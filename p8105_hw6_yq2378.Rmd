---
title: "p8105_hw6_yq2378"
author: "Qi Yumeng"
date: "2023-12-01"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library and seed, warning = FALSE}
set.seed(1)
x = c("tidyverse","ggpubr","modelr","dplyr","MASS","corrplot","mgcv")
lapply(x, require, character.only = TRUE)
```
# Problem 1

```{r load homicide data}
homicide = read.csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")

homicide = homicide |>
  mutate(city_state = str_c(city, state, sep=", "),
         if_solved = if_else(disposition == "Closed by arrest",1,0 ),
         victim_age = as.numeric(victim_age)) |>
  filter(!(city_state %in% c('Dallas, TX', 'Phoenix, AZ', 'Kansas City, MO','Tulsa, AL'))) |>
  filter(victim_race %in% c("White", "Black"),
         !is.na(victim_age),
         victim_sex %in% c("Female", "Male")) # delete non-numeric, missing age and Unknown sex
```

```{r build a function}
# Function to fit logistic regression and extract OR and CI
fit_logistic = function(data) {
  glm_res = glm(if_solved ~ victim_age + victim_sex + victim_race, data = data, family = "binomial")
  tidy_res = broom::tidy(glm_res, exponentiate = TRUE, conf.int = TRUE)
  or_ci = tidy_res |>
    filter(term == "victim_sexMale") |>
    dplyr::select(estimate, conf.low, conf.high)
  return(or_ci)
}
```

```{r Baltimore}
# Fit logistic regression for Baltimore, MD
homicide |>
  filter(city_state == "Baltimore, MD") |>
  fit_logistic()

```

```{r loop the city}
# Fit logistic regression for each city and extract OR and CI
city_results = homicide |>
  group_by(city_state) |>
  nest() |>
  mutate(or_ci = map(data, fit_logistic)) |>
  unnest(or_ci) |>
  dplyr:: select(-data) |>
  rename(OR = estimate, CI_low = conf.low, CI_high = conf.high) |>
  arrange(desc(OR))
```

```{r plot for ORs and CIs, fig.width = 15, fig.align = 'center'}
ggplot(city_results, aes(x = reorder(city_state, OR),
                         y = OR, ymin = CI_low, ymax = CI_high)) +
  geom_point(position = position_dodge(width = 0.5), size = 1, color = "red") +
  geom_errorbar(position = position_dodge(width = 0.5), width = 0.2) +
  geom_hline(yintercept = 1, color = "blue", linetype = 2) +
  labs(title = "Estimated ORs and CIs for Solving Homicides (Male vs. Female)",
       x = "City",
       y = "Odds Ratio") +
  theme_pubclean() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The plot provides a visual representation of the estimated odds ratios (ORs) and their corresponding confidence intervals (CIs) for 47 cities. Notably, New York, NY exhibits the smallest OR, while Albuquerque, NM boasts the largest.

Beginning with Atlanta, GA, a distinct trend emerges: the subsequent six cities exhibit ORs greater than 1. This implies that in these particular cities, homicide cases involving male victims are more likely to be resolved compared to those involving female victims. On the contrary, the remaining 41 cities in the dataset exhibit an inverse relationship, signifying that homicides with female victims tend to have higher odds of resolution.

Regarding the error bars in the plot, it's notable that the cities positioned towards the right side generally exhibit wider error bars. This observation suggests a potential issue related to sample size in these cities. Wider error bars typically indicate increased uncertainty in the estimated odds ratios, and this could be attributed to smaller sample sizes for certain cities.

# Problem 2


```{r load weather data}
weather = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  dplyr::select(name, id, everything())

```

### $\log \left(\hat{\beta}_1 \times \hat{\beta}_2\right)$ Distribution

This distribution has a heavy tail extending to low values and a bit of a “shoulder”. The density plog of $\log \left(\hat{\beta}_1 \times \hat{\beta}_2\right)$ values represents the variability in the logarithmic product of the estimated coefficients for `tmin` and `prxp` in the linear regression model. The value centers around -6.  This quantity captures the joint effect of the predictors on `tmax`. The spread provides insights into the uncertainty associated with this combined predictor influence.


```{r beta prod,fig.align = 'center'}
# prepocessing? 

beta = 
  weather |> 
  modelr::bootstrap(n = 5000) |> 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results = map(models, broom::tidy)) |> 
  dplyr::select(.id,results) |> 
  unnest(results) |> 
  filter(term %in% c('tmin', 'prcp')) |>
  group_by(.id) |>
  summarise(estimate = log(prod(estimate))) 



CI_beta_squared = quantile(beta$estimate, c(0.025, 0.975),na.rm = TRUE)
ggplot(beta,aes(x = estimate)) + geom_density() + theme_pubclean() + 
  labs(title = "Distribution of log(beta1 * beta2)", x = "log(beta1 * beta2)",
       subtitle = paste("95% CI for R-squared: ","(", round(CI_beta_squared[1],2),", ", round(CI_beta_squared[2],2),")", sep ="")) 
```

### $R^2$ Distribution

The distribution of $R^2$ values reflects the variability in how well the linear regression model explains the variance in the maximum temperature. The density plot shows a range of $R^2$ values obtained from 5000 bootstrap samples. It is roughly bell shape. Higher $R^2$ values indicate a better fit of the model, while lower values suggest less explanatory power. The distribution helps us understand the uncertainty in estimating the goodness-of-fit for the given predictors (`tmin` and `prcp`). The most frequent values range from 0.91 to 0.93, which is a pretty good fit.

```{r R2 prod,fig.align = 'center' }
# prepocessing? 
R2 = 
  weather |> 
  modelr::bootstrap(n = 5000) |> 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results = map(models, broom::glance)) |> 
  dplyr::select(results) |> 
  unnest(results) |>
  dplyr::select(r.squared) 

CI_r_squared = quantile(R2$r.squared, c(0.025, 0.975))
ggplot(R2,aes(x = r.squared)) + geom_density() + theme_pubclean() + 
  labs(title = "Distribution of R-squared", x = "R-squared",
       subtitle = paste("95% CI for R-squared: ","(", round(CI_r_squared[1],2),", ", round(CI_r_squared[2],2),")", sep ="")) 
```


# Problem 3

```{r load birthweight data}

variable_factor = c(
  babysex = "factor", 
  frace = "factor",         
  malform = "factor",
  mrace = "factor")

birthweight = read.csv("https://p8105.com/data/birthweight.csv"
                       ,colClasses = variable_factor)
birthweight = birthweight  |> janitor::clean_names()
skimr::skim(birthweight)
# delet pnumlbw and pnumsga
```

There are 4342 observation of 20 variables with no missing value. 4 factors and 16 continuous variables. We delete `pnumlbw` and `pnumsga` for they are all zeros. Next, we standardizaed all the continuous variables. Standardization involves subtracting the mean and dividing by the standard deviation, ensuring that all variables are on a comparable scale. From the correlation plot, we found that some pairs have a correlation higher than 0.8 which may lead to multicollinearity. To avoid this, we remove `ppbmi` and `delwt` leaving only `ppwt`.

```{r scale and correlation,fig.align = 'center'}
# Center non-factor variables
birthweight_scale = birthweight |>
  dplyr::select(-pnumlbw, -pnumsga) |>
  mutate_if(is.numeric, scale)
corrplot(cor(birthweight_scale |> select_if(is.numeric)),  method = "number")
# ppbmi & ppwt 0.85
# ppwt & delwt 0.87

birthweight_scale = birthweight_scale |>
  dplyr::select(-ppbmi, -delwt) 

```


```{r AIC }
fit = lm(bwt~., data = birthweight_scale)
stepAIC(fit, direction="both")
fit_AIC = lm(bwt~babysex + bhead + blength + fincome + gaweeks + mheight + 
    mrace + parity + ppwt + smoken + wtgain, data = birthweight_scale)
summary(fit_AIC)
plot(fit_AIC)
# Plot of model residuals against fitted values
fit_AIC_residuals =  broom::augment(fit_AIC)|>
  add_predictions(fit_AIC) |>
  add_residuals(fit_AIC)

ggplot(fit_AIC_residuals, aes(.fitted, .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs Fitted Values",
       x = "Fitted Values",
       y = "Residuals") +
  theme_minimal()
```

```{r model select}

cv_df =
  crossv_mc(birthweight_scale, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
cv_df = 
  cv_df |> 
  mutate(
    linear_mod  = map(train, \(df) lm(bwt ~ babysex + bhead + blength + fincome + 
                                        gaweeks + mheight + mrace + parity + 
                                        ppwt + smoken + wtgain, data = df)),
    
    alter1_mod = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    alter2_mod  = map(train, \(df) lm(bwt ~ bhead * gaweeks * babysex,  data = df))) |> 
  mutate(
    rmse_linear = map2(linear_mod, test, \(mod, df) modelr::rmse(model = mod, data = df)),
    rmse_alter1    = map2(alter1_mod, test, \(mod, df) modelr::rmse(model = mod, data = df)),
    rmse_alter2 = map2(alter2_mod, test, \(mod, df) modelr::rmse(model = mod, data = df)))
cv_df |> 
  dplyr::select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |>
  mutate(model = factor(model),
         rmse = as.numeric(rmse)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin() + theme_pubclean()

```

